---
title: "KC Housing Prices Truncated"
output:
  html_document:
    df_print: paged
    keep_md : true
  html_notebook: default
---
#Introduction
We will be looking at a dataset of homes sold between May 2014 and May 2015 in King's County in Washington (not DC). Among the cities included is Seattle, the state's largest city. The goal is to predict home prices in the data. This is a work in progress, there's a lot more I want to do before I can call myself happy with what I've done.
```{r}
library(tidyverse)
```

#Data Exploration and Feature Engineering
```{r}
kc <- read_csv("kc_house_data.csv")
```


```{r}
spec(kc)
```
ID may not be too useful since it attains almost as many values as there are housing units. It might useful to know how many times a unit appears in this dataset and use that as a feature, but ID on its own isn't helpful. Date will probably be interesting; we could look for, say, seasonality trends. For now, though, let's not use it. I'm not entirely sure what view is doing... the Kaggle page just says "has been viewed," and I don't know what exactly that means. While zipcode is technically a number, for our purposes it would be better to treat it as a categorical variable. There are a lot of zipcodes, though, which means a lot more parameters in the model:

```{r}
cat("The number of zipcodes: ",length(unique(kc$zipcode)))
```



```{r}
kc2 <- select(kc, -id,-date,-lat,-long)
```

About 60% of housing units have no basement:
```{r}
cat("Percentage of housing units with no basement: ",100*nrow(kc2[kc2$sqft_basement==0,])/nrow(kc2))
```
So, let's replace sqft_basement with a binary representing whether or not there is a basement. We will also  convert zipcode to a categorical.

```{r}
kc2$basement <- as.integer(kc2$sqft_basement==0)
kc2$zipcode <- as.factor(kc2$zipcode)
kc3 <- select(kc2,-sqft_basement)
```

```{r}
kc3
```
Let's make some plots. Since prices span several decades, we should use log(price) in our plots.
###Price versus bedrooms
```{r}
plot(kc3$bedrooms, log(kc3$price),xlab="# Bedrooms",ylab="log(price)", main="Log(price) versus # Bedrooms")
```
Wow, one of these houses has more than thirty bedrooms.
```{r}
max(kc3$bedrooms)
```
Who needs 33 bedrooms?!? Is this a hotel? Let's plot this again without this data point to see if there is a trend.
```{r}
plot(kc3$bedrooms[kc3$bedrooms<33], log(kc3$price[kc3$bedrooms<33]),xlab="# Bedrooms",ylab="log(price)", main="Log(price) versus # Bedrooms" )
```
It looks like it generally increases, although there is a lot of variance. Let's use a box plot:

```{r}
plot(as.factor(kc3$bedrooms), log(kc3$price),xlab="# Bedrooms",ylab="log(price)", main="Log(price) versus # Bedrooms" )
```
The trend is much clearer here.
###Price versus Square Foot Living Space
```{r}
plot(kc3$sqft_living, log(kc3$price),xlab="Square Foot Living",ylab="log(price)", main="Log(price) versus Square Foot Living")
```
We see that price does increase with square footage, but it doesn't quite look linear. Let's plot the log-price per square foot:

```{r}
plot(kc3$sqft_living, log(kc3$price)/kc3$sqft_living,xlab="Square Foot Living",ylab="Log(Price) Per Square Foot", main="Log(price) versus Square Foot Living")
```
Interesting. The price per square foot decreases with high square footage. I woud expect the opposite. From the shape of the plot, I think log(price)/sqft_living ~ 1/sqrt(sqft_living) => log(price)~sqrt(sqft_living). Let's see:  


```{r}
plot(sqrt(kc3$sqft_living), log(kc3$price),xlab="Square Foot Living",ylab="log(price)", main="Log(price) versus Square Foot Living")
```
This looks pretty linear, suggesting sqrt(sqft_living) is a good feature to use in the linear model. I expect the same holds for the other square footage units.


# Modelling the data
Since housing prices range over several decades, we will take a log of prices and use a linear model.
```{r}
fit1 = lm(log(price)~.,data=kc3)
```

```{r}
summary(fit1)
```


```{r}
plot(fit1)
```
We have an r^2 of about 87%. Let's modify the model a bit by adding some features, treating condition and floors as factors, and we'll remove sqft_lot_15.
```{r}
fit2 = update(fit1,log(price)~.+sqrt(sqft_living)+sqrt(sqft_lot)+sqrt(sqft_above)-sqft_lot15+as.factor(floors)-floors-condition+as.factor(condition)+sqrt(sqft_living15 ),data=kc3)
summary(fit2)
```

```{r}
plot(fit2)
```
To make sure this model isn't overfitting, we will break up the data into a training subset and a test subset.
```{r}
smp_size <- floor(0.8 * nrow(kc3))
set.seed(1)
train_ind <- sample(seq_len(nrow(kc3)), size = smp_size)
train <- kc3[train_ind, ]
test <- kc3[-train_ind, ]

fit2.2 <- update(fit2, data=train)
```

```{r}
summary(fit2.2)
```
Let's evaluate the performance by looking at r^2 and RSS.
```{r}
RSS.train <- sum((predict(fit2.2,train) - log(train$price) )^2 )
TSS.train <- sum((mean(log(train$price)) - log(train$price) )^2) 
cat("R-squared for train data: ",1-RSS.train/TSS.train)
```


```{r}
RSS.test <- sum((predict(fit2.2,test) - log(test$price) )^2 )
TSS.test <- sum((mean(log(test$price)) - log(test$price) )^2) 
cat("R-squared for test data: ",1-RSS.test/TSS.test)
```
```{r}
RSS.total <- sum((predict(fit2.2,kc3) - log(kc3$price) )^2 )
TSS.total <- sum((mean(log(kc3$price)) - log(kc3$price) )^2) 
cat("R-squared for all data: ",1-RSS.total/TSS.total)
```


```{r}
cat("RMS for all data: ",sqrt(mean((predict(fit2.2,kc3) - log(kc3$price) )^2 )))
```

The model is performing well with an R-squared of about 88% for all the data as well as the training and test subsets. Our mean squared error is about 0.18. Since we predicted the log of price, we exponentiate our prediction to get actual price, and our error becomes multiplicative, i.e. exp(log(price)+-RMSE) = exp(+-RMSE)*price. Since exp(+RMSE) is about 1.2 and exp(-RMSE) is about 0.84, we're overestimating or underestimating price by about 20% on average.

```{r}
exp(-0.18)
```










